{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b5de05a",
   "metadata": {},
   "source": [
    "# Part 1 - Step-by-step code walkthrough\n",
    "The source of this code and explanation is to be found in the following [article](https://blogs.alisterluiz.com/zero-shot-text-classification-in-8-minutes-a-step-by-step-guide/)\n",
    "\n",
    "- **Importing pipeline:** This high-level function from Hugging Face’s Transformers library simplifies many NLP tasks.\n",
    "- **Setting up the pipeline:** By specifying `\"zero-shot-classification\"` as the task and choosing the facebook/bart-large-mnli model, we’re leveraging a model trained for natural language inference (NLI). This model is adept at comparing a given text with candidate labels to determine the best match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e494977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb288d90db8e4bf9a0c19f256f972dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Esmee Werk\\Documents\\Persoonlijke Projecten\\AI-Onderzoeksvragen\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Esmee Werk\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9061b058404541d09214bd6c74e99ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884925976c254a52a7715635880209a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647ba31487a74b44992e8cdceb5ef56c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018303e97f6e440a8ed548a3be32ad75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f593fd03daf44362a43b0016115c50e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a zero-shot classifier pipeline using Facebook's BART model fine-tuned on MNLI.\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420feb1d",
   "metadata": {},
   "source": [
    "- **Input Text:** This is the text review we want to analyze.\n",
    "- **Candidate Labels:** For this example, we’re classifying the sentiment of the text. The model will score each label based on how well it fits the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34272905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for Sentiment Analysis\n",
    "sequence = \"I absolutely love the new smartphone design – it’s sleek and super intuitive!\"\n",
    "candidate_labels = [\"positive sentiment\", \"negative sentiment\", \"neutral\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89a9cda",
   "metadata": {},
   "source": [
    "- **Running the Classifier:** The classifier function processes the input text along with the candidate labels.\n",
    "- **Output Structure:** The output is a dictionary that includes:\n",
    "    - `labels:` The candidate labels ranked from most to least relevant.\n",
    "    - `scores:` Confidence scores for each label, indicating how likely the text belongs to that category.\n",
    "    This quick output helps you instantly see which sentiment (or label) best matches the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "746eed1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Result:\n",
      "{'sequence': 'I absolutely love the new smartphone design – it’s sleek and super intuitive!', 'labels': ['positive sentiment', 'neutral', 'negative sentiment'], 'scores': [0.9924148321151733, 0.0051809255965054035, 0.0024042511358857155]}\n"
     ]
    }
   ],
   "source": [
    "# Run the classifier and capture the result\n",
    "result = classifier(sequence, candidate_labels)\n",
    "\n",
    "# Print the output\n",
    "print(\"Sentiment Analysis Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ed0a09",
   "metadata": {},
   "source": [
    "You can also use zero-shot classification for different topics but sentiment analysis. You can adapt the same code for topic categorization or any other task by changing the `candidate_labels`.\n",
    "\n",
    "Below we use a new sample text related to AI Advancements.\n",
    "Instead of sentiment, we will provide topic labels. Then the model is going to evaluate which topic best describes the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e70611e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Classification Result:\n",
      "{'sequence': 'The recent advancements in artificial intelligence are transforming industries across the board.', 'labels': ['technology', 'health', 'sports', 'politics'], 'scores': [0.9800008535385132, 0.01203815545886755, 0.004841970279812813, 0.0031191199086606503]}\n"
     ]
    }
   ],
   "source": [
    "# Example for Topic Classification\n",
    "sequence_topic = \"The recent advancements in artificial intelligence are transforming industries across the board.\"\n",
    "candidate_topics = [\"technology\", \"politics\", \"health\", \"sports\"]\n",
    "\n",
    "# Run the classifier for topic classification\n",
    "result_topic = classifier(sequence_topic, candidate_topics)\n",
    "\n",
    "print(\"\\nTopic Classification Result:\")\n",
    "print(result_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258c51f0",
   "metadata": {},
   "source": [
    "## Part 2 - Understanding the technique\n",
    "In this part of the notebook we take the lessons from the article and take it a step further. In part one we used a black box, now we're trying to understand what the black box does.\n",
    "\n",
    "The goal of this part is to:\n",
    "\n",
    "- Explain what `NLI` is (Natural Language Interference)\n",
    "    * The `facebook/bart-large-mnli` is trained on **entail,ent/contradiction/neutral**\n",
    "    * Zero-shot becomes a possibility by transforming labels (like \"sport\") into a 'hypothesis'\n",
    "        - *Premise*: \"The recent advancements in AI are transforming industries.\"\n",
    "        - *Hypothesis*: \"The text is about sports.\"\n",
    "        - The model will decide if Premise or Hypothesis entails, contradicts or is neutral.\n",
    "- I will show a \"handmade\" NLI without pipeline\n",
    "    - For this i will use Hugging Face `AutoTokenizer` and `AutoModelForSequenceClassification` to tokenize my own input en take logits from the model.\n",
    "    - This shows how you get the probabilities for entailment/contradiction/neutral\n",
    "    - I will apply this on my own “sports/technology/health” labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6938ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"facebook/bart-large-mnli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb1f1c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis: 'This text is about technology.'\n",
      "Entailment probability: 0.56221676\n",
      "----------------------------------------\n",
      "Hypothesis: 'This text is about sports.'\n",
      "Entailment probability: 0.00018156749\n",
      "----------------------------------------\n",
      "Hypothesis: 'This text is about politics.'\n",
      "Entailment probability: 0.00016065556\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "premise = \"The recent advancements in AI are transforming industries.\"\n",
    "hypotheses = [\n",
    "    \"This text is about technology.\",\n",
    "    \"This text is about sports.\",\n",
    "    \"This text is about politics.\"\n",
    "]\n",
    "\n",
    "for hypothesis in hypotheses:\n",
    "    inputs = tokenizer(premise, hypothesis, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = torch.softmax(logits, dim=1).detach().numpy()\n",
    "    entailment_prob = probs[0][2]  # label 2 is usually entailment\n",
    "    print(f\"Hypothesis: '{hypothesis}'\")\n",
    "    print(\"Entailment probability:\", entailment_prob)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94f2005",
   "metadata": {},
   "source": [
    "Since the premise and hypothesis concept is still a bit vague heres an in-depth explanation.\n",
    "\n",
    "A NLI-Model always gets two scentences:\n",
    "\n",
    "1. **Premise**: This is the given text (what we know) --> \"*The recent advancements in AI are transforming industries*\"\n",
    "2. **Hypothesis**: This is the proposition we want to check.\n",
    "\n",
    "The model will then decide if the **hypothesis** is logical according to the **premies**.\n",
    "\n",
    "This can give you three results:\n",
    "\n",
    "* **Entailment**: The hypothesis flows logically from the premise\n",
    "* **Contradiction**: The hypothesis contradicts the premise\n",
    "* **Neutral**: The hypothesis can't be confirmed nor denied based on the premise\n",
    "\n",
    "Below i will show three examples of entailment, contradiction and neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05a19a34",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: A man is playing a guitar on stage.\n",
      "Hypothesis: A person is performing music.\n",
      "Predicted: Entailment (probabilities: [np.float32(0.01), np.float32(0.37), np.float32(99.62)]%)\n",
      "Expected: Entailment\n",
      "----------------------------------------\n",
      "Premise: A man is playing a guitar on stage.\n",
      "Hypothesis: A person is cooking dinner.\n",
      "Predicted: Contradiction (probabilities: [np.float32(99.98), np.float32(0.01), np.float32(0.0)]%)\n",
      "Expected: Contradiction\n",
      "----------------------------------------\n",
      "Premise: A man is playing a guitar on stage.\n",
      "Hypothesis: A person is feeling happy.\n",
      "Predicted: Neutral (probabilities: [np.float32(1.05), np.float32(97.56), np.float32(1.39)]%)\n",
      "Expected: Neutral\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"premise\": \"A man is playing a guitar on stage.\",\n",
    "        \"hypothesis\": \"A person is performing music.\",\n",
    "        \"expected\": \"Entailment\"\n",
    "    },\n",
    "    {\n",
    "        \"premise\": \"A man is playing a guitar on stage.\",\n",
    "        \"hypothesis\": \"A person is cooking dinner.\",\n",
    "        \"expected\": \"Contradiction\"\n",
    "    },\n",
    "    {\n",
    "        \"premise\": \"A man is playing a guitar on stage.\",\n",
    "        \"hypothesis\": \"A person is feeling happy.\",\n",
    "        \"expected\": \"Neutral\"\n",
    "    }\n",
    "]\n",
    "\n",
    "labels = [\"Contradiction\", \"Neutral\", \"Entailment\"]\n",
    "\n",
    "for ex in examples:\n",
    "    inputs = tokenizer(ex[\"premise\"], ex[\"hypothesis\"], return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = torch.softmax(logits, dim=1).detach().numpy()[0]\n",
    "    probs_percent = [round(p * 100, 2) for p in probs]\n",
    "    predicted = labels[probs.argmax()]\n",
    "    print(f\"Premise: {ex['premise']}\")\n",
    "    print(f\"Hypothesis: {ex['hypothesis']}\")\n",
    "    print(f\"Predicted: {predicted} (probabilities: {probs_percent}%)\")\n",
    "    print(f\"Expected: {ex['expected']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d95544",
   "metadata": {},
   "source": [
    "#### How can this be used for Zero-Shot learning?\n",
    "With classification you usually have a set list of labels (example:[\"sports\", \"politics\", \"technology\"])\n",
    "\n",
    "With zero-shot you **don't** have a trained model for these specific labels. See the following example:\n",
    "\n",
    "Premise: \"*The recent advancements in artificial intelligence are transforming industries.*\"\n",
    "\n",
    "- **Hypothesis 1:** \"This text is about sports.\"\n",
    "- **Hypothesis 2:** \"This text is about technology.\"\n",
    "- **Hypothesis 3:** \"This text is about politics.\"\n",
    "\n",
    "The NLI-Model will then check per hypothesis: \"How probable is entailment?\"\n",
    "    - If the entailment-score is high for \"technology\", the model will then say:\n",
    "        * this text is probably about technology\n",
    "\n",
    "**Why this works**\n",
    "The NLI-Model is trained to understand **relationships between scentences**. You re-use those skills to do new classification-tasks without extra training. This is why a BART-MNLI can predict labels its never seen before: zero-shot learning.\n",
    "\n",
    "---\n",
    "\n",
    "### Summarized\n",
    "\n",
    "* **Premise:** Your input sentencen\n",
    "* **Hypothesis**: \"This text is about [label]\"\n",
    "* **Entailment**: How probable the model thinks that the premise supports the hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b5dcdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
