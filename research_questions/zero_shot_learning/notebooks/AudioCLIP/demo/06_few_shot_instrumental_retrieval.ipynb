{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11bbf255",
   "metadata": {},
   "source": [
    "AudioCLIP Few-Shot Instrument Classification Demo\n",
    "\n",
    "Based on the original [AudioCLIP repository by Andrey Guzhov](https://github.com/AndreyGuzhov)\n",
    "\n",
    "This notebook shows how to use a pretrained AudioCLIP model to recognize musical instruments from the IRMAS dataset using few-shot learning.\n",
    "\n",
    "Few-shot learning tests how well a model performs when it sees only a few labeled examples of each class.\n",
    "Instead of retraining the model, we create support sets with a few samples per instrument and measure how well the model classifies query samples using simple similarity between their embeddings.\n",
    "This approach evaluates how well the pretrained model transfers to new, unseen classes with minimal supervision.\n",
    "\n",
    "---\n",
    "\n",
    "# Notebook Outline\n",
    "### Setup & Preparation\n",
    "\n",
    "1. [Imports and Repo Path](#Imports-and-repo-path)\n",
    "2. [Paths, Downloads, and Config](#Paths,-downloads,-and-config)\n",
    "3. [IRMAS Dataset Scan and Splits](#IRMAS-dataset-scan-and-splits)\n",
    "\n",
    "### Model and Embedding Pipeline\n",
    "\n",
    "4. [Model Load and Transform](#Model-load-and-transform)\n",
    "5. [Audio Preprocessing and Encoding](#Audio-preprocessing-and-encoding)\n",
    "6. [Embed Train and Test Splits](#Embed-train-and-test-splits)\n",
    "\n",
    "### Few-Shot Evaluation\n",
    "\n",
    "7. [Few-Shot Setup and Helpers](#Few-shot-setup-and-helper-functions)\n",
    "8. [One Cross-Split Episode](#One-cross-split-episode)\n",
    "9. [Aggregate Results Across Episodes](#Aggregate-results-over-many-episodes)\n",
    "\n",
    "### Experimenting\n",
    "10. [Instrument Detection](#Instrument-Detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcde309f",
   "metadata": {},
   "source": [
    "# Imports and repo path\n",
    "\n",
    "• Import libraries for files, math, and data: os, sys, gc, urllib, Path, numpy, pandas, torch, librosa, matplotlib, sklearn.\n",
    "• Extend the Python path so your notebook sees local modules.\n",
    "• Load project modules: AudioCLIP and ToTensor1D.\n",
    "\n",
    "#### short summary\n",
    "\n",
    "Load all tools and prepare your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dbdf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & repo path ------------------------------------------------------\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import urllib.request\n",
    "from pathlib import Path, Path as _P\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# If your project modules live one directory up:\n",
    "sys.path.append(os.path.abspath(f'{os.getcwd()}/..'))\n",
    "\n",
    "# Project-local imports\n",
    "from model import AudioCLIP\n",
    "from utils.transforms import ToTensor1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581056bb",
   "metadata": {},
   "source": [
    "# Paths, downloads, and config\n",
    "\n",
    "• Define asset and data directories.\n",
    "• Check if IRMAS data folders exist.\n",
    "• Define model path and download if missing.\n",
    "• Set audio constants for sample rate, segment length, and batch size.\n",
    "• Detect GPU or CPU.\n",
    "\n",
    "short summary\n",
    "\n",
    "• Set file locations, prepare the model, and define global settings.# Build the IRMAS dataframe (scan WAVs + read labels)\n",
    "\n",
    "We walk the IRMAS folder, find every .wav, and look for a matching .txt file with labels. IRMAS often uses 3-letter abbreviations (e.g., gel for electric guitar), so we map those to full names. The result is a DataFrame with absolute file paths and both abbreviated and full labels. We also shuffle with a fixed seed to keep results reproducible.\n",
    "\n",
    "#### short summary\n",
    "\n",
    "* Collect all .wav files and their label .txt\n",
    "* Convert short tags to full instrument names\n",
    "* Produce a tidy df we can work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5557857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paths, asset download, and global config --------------------------------\n",
    "ASSETS_DIR = Path(\"../assets\"); ASSETS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_ROOTS = [\n",
    "    Path(\"data/IRMAS/IRMAS-TestingData-Part1/Part1\"),\n",
    "    Path(\"data/IRMAS/IRMAS-TestingData-Part2/Part2\"),\n",
    "    Path(\"data/IRMAS/IRMAS-TestingData-Part3/Part3\"),\n",
    "    Path(\"data/IRMAS/IRMAS-TrainingData\"),\n",
    "]\n",
    "for r in DATA_ROOTS:\n",
    "    assert r.exists(), f\"Missing: {r.resolve()}\"\n",
    "\n",
    "MODEL_FILE = ASSETS_DIR / \"AudioCLIP-Full-Training.pt\"\n",
    "MODEL_URL  = \"https://github.com/AndreyGuzhov/AudioCLIP/releases/download/v0.1/AudioCLIP-Full-Training.pt\"\n",
    "\n",
    "def _maybe_dl(url: str, out_path: Path):\n",
    "    if not out_path.exists():\n",
    "        print(f\"Downloading {out_path.name} …\")\n",
    "        urllib.request.urlretrieve(url, out_path)\n",
    "    else:\n",
    "        print(f\"{out_path.name} already exists.\")\n",
    "\n",
    "_maybe_dl(MODEL_URL, MODEL_FILE)\n",
    "\n",
    "# Audio/embedding config\n",
    "SAMPLE_RATE     = 44100\n",
    "SEGMENT_SEC     = 10          # IRMAS clips fit in 10s\n",
    "BATCH_SIZE      = 16          # lower if memory is tight\n",
    "WINDOW_POOLING  = False       # True = multi-window averaging for long clips\n",
    "HOP_SEC         = 2.0         # hop between windows if WINDOW_POOLING=True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6f58a3",
   "metadata": {},
   "source": [
    "# IRMAS dataset scan and splits\n",
    "\n",
    "* Map short instrument codes to full names.\n",
    "* Read text label files safely using UTF-8 or Latin-1.\n",
    "* Build a dataframe listing every WAV file, its labels, and its split (train or test).\n",
    "* Use absolute paths so audio loading works everywhere.\n",
    "* Shuffle rows for random order.\n",
    "* Keep only clips with one label.\n",
    "* Split data into train and test sets.\n",
    "* Keep the label names that exist in both sets.\n",
    "\n",
    "#### short summary\n",
    "\n",
    "Build a clean, labeled dataset and prepare train and test lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca478ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IRMAS dataset scanning & splits -----------------------------------------\n",
    "ABBR_TO_FULL = {\n",
    "    \"cel\": \"cello\", \"cla\": \"clarinet\", \"flu\": \"flute\", \"gac\": \"acoustic guitar\",\n",
    "    \"gel\": \"electric guitar\", \"org\": \"organ\", \"pia\": \"piano\", \"sax\": \"saxophone\",\n",
    "    \"tru\": \"trumpet\", \"vio\": \"violin\", \"voi\": \"voice (singing)\",\n",
    "}\n",
    "\n",
    "def _read_txt_labels(txt_path: Path):\n",
    "    try:\n",
    "        try:\n",
    "            text = txt_path.read_text(encoding=\"utf-8\")\n",
    "        except UnicodeDecodeError:\n",
    "            text = txt_path.read_text(encoding=\"latin-1\")\n",
    "        parts = []\n",
    "        for line in text.splitlines():\n",
    "            parts += [p.strip() for p in line.replace(\",\", \" \").split() if p.strip()]\n",
    "        return parts\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not read {txt_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def _abbr2full(seq):\n",
    "    return [ABBR_TO_FULL.get(x, x) for x in seq]\n",
    "\n",
    "def build_irmas_df_multi(roots: List[Path]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for root in roots:\n",
    "        is_training = \"trainingdata\" in str(root).lower() or \"train\" in str(root).lower()\n",
    "        wavs = sorted(root.rglob(\"*.wav\"))\n",
    "        print(f\"[INFO] {root} -> found {len(wavs)} wavs; is_training={is_training}\")\n",
    "        for w in wavs:\n",
    "            abs_path = w.resolve()  # absolute path; robust for librosa\n",
    "            if is_training:\n",
    "                abbr = w.parent.name.lower()\n",
    "                full = ABBR_TO_FULL.get(abbr, abbr)\n",
    "                rows.append({\n",
    "                    \"filepath\": str(abs_path),\n",
    "                    \"labels_abbr\": [abbr],\n",
    "                    \"labels_full\": [full],\n",
    "                    \"source_split\": \"train\",\n",
    "                })\n",
    "            else:\n",
    "                txt = w.with_suffix(\".txt\")\n",
    "                labs_abbr = _read_txt_labels(txt) if txt.exists() else []\n",
    "                labs_full = _abbr2full(labs_abbr) if labs_abbr else []\n",
    "                rows.append({\n",
    "                    \"filepath\": str(abs_path),\n",
    "                    \"labels_abbr\": labs_abbr,\n",
    "                    \"labels_full\": labs_full,\n",
    "                    \"source_split\": \"test\",\n",
    "                })\n",
    "    df = pd.DataFrame(rows, columns=[\"filepath\", \"labels_abbr\", \"labels_full\", \"source_split\"])\n",
    "    df = df.sample(frac=1.0, random_state=0).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Build the index\n",
    "df = build_irmas_df_multi(DATA_ROOTS)\n",
    "print(\"Total WAVs:\", len(df))\n",
    "flat = [x for L in df[\"labels_full\"] for x in (L if isinstance(L, list) else [])]\n",
    "print(\"Unique instruments:\", sorted(set(flat))[:20], \"…\")\n",
    "\n",
    "# Single-label subsets per split\n",
    "df_single = df.copy()\n",
    "df_single[\"label_single\"] = df_single[\"labels_full\"].apply(\n",
    "    lambda L: L[0] if isinstance(L, list) and len(L) == 1 else None\n",
    ")\n",
    "df_single = df_single[df_single[\"label_single\"].notna()].reset_index(drop=True)\n",
    "\n",
    "df_train = df_single[df_single[\"source_split\"] == \"train\"].reset_index(drop=True)\n",
    "df_test  = df_single[df_single[\"source_split\"] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train single-label: {len(df_train)}  | Test single-label: {len(df_test)}\")\n",
    "common_labels = sorted(set(df_train[\"label_single\"]).intersection(df_test[\"label_single\"]))\n",
    "print(\"Common labels train∩test:\", common_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9541b5",
   "metadata": {},
   "source": [
    "# Model load and transform\n",
    "\n",
    "* *Disable gradients for faster inference.\n",
    "* Load the pretrained AudioCLIP model and move it to your device.\n",
    "* Set evaluation mode so weights stay frozen.\n",
    "* Create a transform to convert raw audio arrays into tensors.\n",
    "\n",
    "#### short summary\n",
    "\n",
    "Load the AudioCLIP model and set up audio preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf2a660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model load & transforms --------------------------------------------------\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Load pretrained AudioCLIP\n",
    "aclp = AudioCLIP(pretrained=str(MODEL_FILE)).to(device)\n",
    "aclp.eval()\n",
    "aclp.requires_grad_(False)  # belt-and-suspenders\n",
    "\n",
    "# 1D audio transform expected by the model\n",
    "audio_transforms = ToTensor1D()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca90b08c",
   "metadata": {},
   "source": [
    "# Audio preprocessing and encoding\n",
    "\n",
    "* Define `load_seg_center` to read audio, fix length, and center the segment.\n",
    "* Define `audio_embed_single` to run one clip through AudioCLIP and normalize the output.\n",
    "* Define `embed_file_multiwin` to process long clips in overlapping windows and average embeddings.\n",
    "* Define `encode_audio_in_batches` to load many clips in batches, compute embeddings, normalize, and join results.\n",
    "\n",
    "##### short summary\n",
    "\n",
    "Turn audio clips into normalized embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eb4f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Audio preprocessing & batched encoding ----------------------------------\n",
    "def load_seg_center(path, sr=SAMPLE_RATE, seconds=SEGMENT_SEC):\n",
    "    x, _ = librosa.load(path, sr=sr, mono=True, dtype=np.float32)\n",
    "    win = int(seconds * sr)\n",
    "    if len(x) == 0:\n",
    "        return np.zeros(win, dtype=np.float32)\n",
    "    if len(x) < win:\n",
    "        reps = int(np.ceil(win / len(x)))\n",
    "        x = np.tile(x, reps)[:win]\n",
    "    else:\n",
    "        start = max(0, (len(x) - win) // 2)\n",
    "        x = x[start:start + win]\n",
    "    return x\n",
    "\n",
    "@torch.no_grad()\n",
    "def audio_embed_single(track_np: np.ndarray) -> np.ndarray:\n",
    "    t = audio_transforms(track_np.reshape(1, -1))\n",
    "    ((a_feat, _, _), _), _ = aclp(audio=t.unsqueeze(0))  # [1,D]\n",
    "    a = a_feat[0]\n",
    "    a = a / a.norm()\n",
    "    return a.cpu().numpy()\n",
    "\n",
    "def embed_file_multiwin(path, sr=SAMPLE_RATE, seconds=SEGMENT_SEC, hop_sec=HOP_SEC):\n",
    "    x, _ = librosa.load(path, sr=sr, mono=True, dtype=np.float32)\n",
    "    win = int(seconds * sr)\n",
    "    hop = int(hop_sec * sr)\n",
    "    if len(x) < win:\n",
    "        return audio_embed_single(load_seg_center(path, sr=sr, seconds=seconds))\n",
    "    embs = []\n",
    "    for s in range(0, len(x) - win + 1, hop):\n",
    "        seg = x[s:s + win]\n",
    "        embs.append(audio_embed_single(seg))\n",
    "    E = np.stack(embs, 0)          # [num_windows, D]\n",
    "    v = E.mean(0)\n",
    "    return v / np.linalg.norm(v)\n",
    "\n",
    "def encode_audio_in_batches(paths: List[str], batch_size=BATCH_SIZE) -> torch.Tensor:\n",
    "    feats = []\n",
    "    for i in range(0, len(paths), batch_size):\n",
    "        chunk = paths[i:i + batch_size]\n",
    "        if WINDOW_POOLING:\n",
    "            arr = np.stack([embed_file_multiwin(p) for p in chunk])  # [B, D]\n",
    "            feats.append(torch.from_numpy(arr))\n",
    "        else:\n",
    "            tracks = [load_seg_center(p, sr=SAMPLE_RATE) for p in chunk]\n",
    "            batch = torch.stack([audio_transforms(t.reshape(1, -1)) for t in tracks])  # [B,1,T]\n",
    "            with torch.inference_mode():\n",
    "                ((a_feat, _, _), _), _ = aclp(audio=batch.to(device))                 # [B,D]\n",
    "                a_feat = a_feat / torch.linalg.norm(a_feat, dim=-1, keepdim=True)    # L2\n",
    "            feats.append(a_feat.detach().cpu())\n",
    "            del tracks, batch, a_feat\n",
    "        gc.collect()\n",
    "    return torch.cat(feats, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9c2a1e",
   "metadata": {},
   "source": [
    "# Embed train and test splits\n",
    "\n",
    "* Define `build_feats` to process all clips in a dataframe.\n",
    "* Encode each file into an embedding matrix.\n",
    "* Build a class list and label-to-index mapping.\n",
    "* Convert text labels to numeric arrays.\n",
    "* Compute embeddings for train and test sets.\n",
    "* Print tensor shapes to confirm success.\n",
    "\n",
    "##### short summary\n",
    "\n",
    "Create embeddings and label maps for train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921b765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Embed train/test splits --------------------------------------------------\n",
    "def build_feats(df_any: pd.DataFrame):\n",
    "    paths = df_any[\"filepath\"].tolist()\n",
    "    A = encode_audio_in_batches(paths, batch_size=BATCH_SIZE)   # already L2-normalized\n",
    "    class_names = sorted(df_any[\"label_single\"].unique())\n",
    "    cls2idx = {c: i for i, c in enumerate(class_names)}\n",
    "    y = df_any[\"label_single\"].map(cls2idx).to_numpy()\n",
    "    return A, y, class_names, cls2idx\n",
    "\n",
    "A_train, y_train, class_names_train, cls2idx_train = build_feats(df_train)\n",
    "A_test,  y_test,  class_names_test,  cls2idx_test  = build_feats(df_test)\n",
    "\n",
    "print(\"A_train:\", tuple(A_train.shape), \"| A_test:\", tuple(A_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5fd606",
   "metadata": {},
   "source": [
    "# Few-shot setup and helper functions\n",
    "\n",
    "* Define `constants` for ways, shots, queries, and number of episodes.\n",
    "* `sample_episode_cross_split` picks random classes and selects support and query samples.\n",
    "* `pretty_print_episode` shows each query, its true label, and top predictions with confidence.\n",
    "\n",
    "#### short summary\n",
    "\n",
    "Define episode sampling and print top predictions in readable form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee2cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WAY       = 5\n",
    "K_SHOT      = 5\n",
    "Q_PER_CLASS = 8\n",
    "EPISODES    = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13397c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Few-shot config, samplers, and pretty-printer ---------------------------\n",
    "def sample_episode_cross_split(df_train, df_test, common_labels,\n",
    "                               n_way=5, k_shot=5, q_per_class=8, rng=np.random.default_rng(0)):\n",
    "    if len(common_labels) == 0:\n",
    "        raise ValueError(\"No common labels between train and test.\")\n",
    "    chosen = rng.choice(common_labels, size=min(n_way, len(common_labels)), replace=False)\n",
    "    s_idx, q_idx = [], []\n",
    "    for lbl in chosen:\n",
    "        tr = df_train.index[df_train[\"label_single\"] == lbl].to_numpy()\n",
    "        te = df_test.index[df_test[\"label_single\"] == lbl].to_numpy()\n",
    "        if len(tr) < k_shot or len(te) < q_per_class:\n",
    "            continue\n",
    "        s_idx.extend(rng.choice(tr, size=k_shot, replace=False))\n",
    "        q_idx.extend(rng.choice(te, size=q_per_class, replace=False))\n",
    "    return np.array(s_idx), np.array(q_idx), list(chosen)\n",
    "\n",
    "def pretty_print_episode(S, true, epi_classes, class_names, query_idx, df_ref, topk=3, max_print=20):\n",
    "    confidence = torch.softmax(S, dim=1)  # [Q, C_epi]\n",
    "    print(f\"\\n{'Filename':<42} | {'True label':<25} | Top-{topk} predictions (confidence)\")\n",
    "    print(\"-\" * 110)\n",
    "    for i in range(min(len(query_idx), max_print)):\n",
    "        k = min(topk, confidence.size(1))\n",
    "        conf_values, ids = confidence[i].topk(k)\n",
    "        preds = [\n",
    "            f\"{class_names[epi_classes[j]]} ({conf_values[k_].item() * 100:5.2f}%)\"\n",
    "            for k_, j in enumerate(ids)\n",
    "        ]\n",
    "        fname = _P(df_ref.iloc[query_idx[i]][\"filepath\"]).name\n",
    "        fname = (fname[:40] + \"…\") if len(fname) > 41 else fname\n",
    "        true_lbl = df_ref.iloc[query_idx[i]][\"label_single\"]\n",
    "        print(f\"{fname:<42} | {true_lbl:<25} | {'; '.join(preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28d2a42",
   "metadata": {},
   "source": [
    "# One cross-split episode\n",
    "\n",
    "* Sample one episode of data.\n",
    "* Define `proto_predict_cross` to compute class prototypes and classify queries.\n",
    "* Average support embeddings per class and normalize.\n",
    "* Compute cosine similarities and predict the closest prototype.\n",
    "* Map local predictions to global labels.\n",
    "* Print accuracy, plot confusion matrix, and display predictions.\n",
    "\n",
    "#### short summary\n",
    "\n",
    "Run one few-shot test, compute accuracy, and show predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8d7a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- One cross-split episode: prototypes, prediction, report -----------------\n",
    "rng = np.random.default_rng(42)\n",
    "s_idx, q_idx, chosen = sample_episode_cross_split(\n",
    "    df_train, df_test, common_labels,\n",
    "    n_way=N_WAY, k_shot=K_SHOT, q_per_class=Q_PER_CLASS, rng=rng\n",
    ")\n",
    "\n",
    "@torch.no_grad()\n",
    "def proto_predict_cross(A_train, df_train, A_test, df_test, s_idx, q_idx, chosen_labels):\n",
    "    # local class space\n",
    "    class_names = sorted(chosen_labels)\n",
    "    cls2idx = {c: i for i, c in enumerate(class_names)}\n",
    "\n",
    "    # Map labels to local ids; unknowns -> -1\n",
    "    y_train_local = df_train[\"label_single\"].map(lambda x: cls2idx.get(x, -1)).to_numpy(dtype=int)\n",
    "    y_test_local  = df_test[\"label_single\"].map(lambda x: cls2idx.get(x, -1)).to_numpy(dtype=int)\n",
    "\n",
    "    # select episode data\n",
    "    sup_y = y_train_local[s_idx].astype(int)           # [S]\n",
    "    sup_X = A_train[s_idx]                             # [S, D]\n",
    "\n",
    "    # prototypes\n",
    "    classes_epi = np.unique(sup_y)                     # local ids (ints)\n",
    "    protos = []\n",
    "    for ci in classes_epi:\n",
    "        mask = torch.from_numpy((sup_y == ci).astype(np.float32)).unsqueeze(1)  # [S,1]\n",
    "        proto = (sup_X * mask).sum(dim=0) / mask.sum()                           # [D]\n",
    "        proto = proto / proto.norm()\n",
    "        protos.append(proto)\n",
    "    P = torch.stack(protos, 0)                         # [C_epi, D]\n",
    "\n",
    "    # score queries\n",
    "    qry_X = A_test[q_idx]                              # [Q, D]\n",
    "    S = qry_X @ P.T                                    # [Q, C_epi]\n",
    "    pred_local = S.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "    classes_epi = classes_epi.astype(int)\n",
    "    pred_global = classes_epi[pred_local]\n",
    "    true_global = y_test_local[q_idx].astype(int)\n",
    "\n",
    "    return pred_global, true_global, S, classes_epi, class_names\n",
    "\n",
    "pred, true, S_epi, epi_classes, class_names_epi = proto_predict_cross(\n",
    "    A_train, df_train, A_test, df_test, s_idx, q_idx, chosen\n",
    ")\n",
    "\n",
    "acc = accuracy_score(true, pred)\n",
    "print(f\"Cross-split prototype — Top-1 accuracy: {acc:.3f}\")\n",
    "\n",
    "epi_classes = epi_classes.astype(int)\n",
    "labels_epi = [class_names_epi[i] for i in epi_classes]\n",
    "cm = confusion_matrix(true, pred, labels=epi_classes)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=labels_epi)\n",
    "disp.plot(xticks_rotation=45, values_format='d', cmap=\"viridis\")\n",
    "plt.show()\n",
    "\n",
    "pretty_print_episode(S_epi, true, epi_classes, class_names_epi, q_idx, df_test, topk=2, max_print=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c395c4",
   "metadata": {},
   "source": [
    "# Aggregate results over many episodes\n",
    "\n",
    "* Define `ALL_NAMES` and `ALL_IDX` as the shared label space.\n",
    "* `run_fewshot_confmat_cross` repeats few-shot tests across many episodes.\n",
    "* Collect all true and predicted labels in global form.\n",
    "* Build one large confusion matrix from all episodes.\n",
    "* Plot the aggregated confusion matrix to inspect global accuracy patterns.\n",
    "\n",
    "#### short summary\n",
    "\n",
    "• Run many episodes and combine results into a global confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Aggregate confusion matrix across many episodes --------------------------\n",
    "from tqdm import trange\n",
    "\n",
    "# Global label space = all labels that exist in both splits\n",
    "ALL_NAMES = sorted(common_labels)\n",
    "ALL_IDX   = {c: i for i, c in enumerate(ALL_NAMES)}\n",
    "\n",
    "def run_fewshot_confmat_cross(n_episodes=100, n_way=5, k_shot=5, q_per_class=8, seed=0):\n",
    "    \"\"\"\n",
    "    Runs multiple cross-split few-shot episodes and aggregates all query predictions\n",
    "    into a single confusion matrix label space (ALL_NAMES).\n",
    "    Returns: y_true_all_global, y_pred_all_global as int arrays.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    y_true_all, y_pred_all = [], []\n",
    "\n",
    "    for _ in trange(n_episodes, desc=\"Episodes\"):\n",
    "        s_idx, q_idx, chosen = sample_episode_cross_split(\n",
    "            df_train, df_test, common_labels,\n",
    "            n_way=n_way, k_shot=k_shot, q_per_class=q_per_class, rng=rng\n",
    "        )\n",
    "        if len(s_idx) == 0 or len(q_idx) == 0:\n",
    "            continue\n",
    "\n",
    "        pred_local, true_local, _, epi_classes, class_names_epi = proto_predict_cross(\n",
    "            A_train, df_train, A_test, df_test, s_idx, q_idx, chosen\n",
    "        )\n",
    "\n",
    "        pred_local = np.asarray(pred_local, dtype=int)\n",
    "        true_local = np.asarray(true_local, dtype=int)\n",
    "\n",
    "        # Map local ids -> label names -> global ids\n",
    "        y_true_names = [class_names_epi[i] for i in true_local]\n",
    "        y_pred_names = [class_names_epi[i] for i in pred_local]\n",
    "        y_true_global = [ALL_IDX[n] for n in y_true_names]\n",
    "        y_pred_global = [ALL_IDX[n] for n in y_pred_names]\n",
    "\n",
    "        y_true_all.extend(y_true_global)\n",
    "        y_pred_all.extend(y_pred_global)\n",
    "\n",
    "    return np.array(y_true_all, dtype=int), np.array(y_pred_all, dtype=int)\n",
    "\n",
    "# Run aggregation\n",
    "y_true_all, y_pred_all = run_fewshot_confmat_cross(\n",
    "    n_episodes=100, n_way=5, k_shot=5, q_per_class=8, seed=123\n",
    ")\n",
    "\n",
    "print(f\"Totaal aantal query samples geëvalueerd: {len(y_true_all)}\")\n",
    "\n",
    "cm = confusion_matrix(y_true_all, y_pred_all, labels=range(len(ALL_NAMES)))\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=ALL_NAMES)\n",
    "disp.plot(xticks_rotation=90, cmap=\"viridis\", values_format=\"d\")\n",
    "plt.title(\"Cross-Split Few-Shot — Aggregated Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeffcfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build class prototypes from TRAIN (few-shot) -----------------------------\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def build_prototypes_from_train(df_train, A_train,\n",
    "                                target_labels=None, k_support=5, seed=0):\n",
    "    \"\"\"\n",
    "    target_labels: list of instrument names to include (default = ALL_NAMES)\n",
    "    k_support:     number of support clips per class (few-shot)\n",
    "    Returns:\n",
    "      P: torch [C, D] prototypes (L2-normalized)\n",
    "      class_names: list[str] in same order as rows of P\n",
    "      used_idx: dict[label] -> indices (in df_train) used as support\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    if target_labels is None:\n",
    "        target_labels = list(ALL_NAMES)\n",
    "\n",
    "    class_names = []\n",
    "    protos = []\n",
    "    used_idx = {}\n",
    "\n",
    "    for lbl in sorted(target_labels):\n",
    "        idxs = df_train.index[df_train[\"label_single\"] == lbl].to_numpy()\n",
    "        if len(idxs) == 0:\n",
    "            print(f\"[skip] No training examples for '{lbl}'\")\n",
    "            continue\n",
    "        take = min(k_support, len(idxs))\n",
    "        chosen = rng.choice(idxs, size=take, replace=False)\n",
    "        used_idx[lbl] = chosen\n",
    "\n",
    "        Xs = A_train[chosen]                     # [S, D] embeddings from train\n",
    "        proto = Xs.mean(dim=0)\n",
    "        proto = proto / proto.norm()\n",
    "        protos.append(proto)\n",
    "        class_names.append(lbl)\n",
    "\n",
    "    P = torch.stack(protos, dim=0) if len(protos) else torch.empty(0)\n",
    "    return P, class_names, used_idx\n",
    "\n",
    "# Example: build 5-shot prototypes for all labels you evaluated\n",
    "P, proto_labels, support_used = build_prototypes_from_train(df_train, A_train,\n",
    "                                                            target_labels=ALL_NAMES,\n",
    "                                                            k_support=5, seed=123)\n",
    "print(\"Prototypes:\", P.shape, \"| classes:\", proto_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832964e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Windowed embedding + scoring for a new audio file -----------------------\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def embed_windows(path, sr=SAMPLE_RATE, seconds=SEGMENT_SEC, hop_sec=2.0):\n",
    "    \"\"\"\n",
    "    Return per-window embeddings for a file.\n",
    "    Outputs:\n",
    "      E: torch [W, D] L2-normalized embeddings\n",
    "      times: list of (start_sec, end_sec) per window\n",
    "    \"\"\"\n",
    "    x, _ = librosa.load(path, sr=sr, mono=True, dtype=np.float32)\n",
    "    win = int(seconds * sr)\n",
    "    hop = int(hop_sec * sr)\n",
    "\n",
    "    if len(x) < win:\n",
    "        # pad by tiling (same as center loader but simpler for timing)\n",
    "        reps = int(np.ceil(win / max(1, len(x))))\n",
    "        x = np.tile(x, reps)[:win]\n",
    "\n",
    "    embs = []\n",
    "    times = []\n",
    "    for s in range(0, max(1, len(x) - win + 1), hop):\n",
    "        seg = x[s:s+win]\n",
    "        if len(seg) < win:  # pad last partial window\n",
    "            seg = np.pad(seg, (0, win - len(seg)))\n",
    "        t = audio_transforms(seg.reshape(1, -1)).unsqueeze(0)  # [1,1,T]\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            ((a_feat, _, _), _), _ = aclp(audio=t.to(next(aclp.parameters()).device))\n",
    "            a_feat = a_feat / torch.linalg.norm(a_feat, dim=-1, keepdim=True)\n",
    "        embs.append(a_feat.cpu())\n",
    "        times.append((s / sr, (s + win) / sr))\n",
    "\n",
    "    E = torch.cat(embs, dim=0) if embs else torch.empty(0)\n",
    "    return E, times\n",
    "\n",
    "def score_file_against_prototypes(audio_path, P, class_names, hop_sec=2.0, seg_sec=SEGMENT_SEC):\n",
    "    \"\"\"\n",
    "    Compute cosine scores for each window vs each prototype.\n",
    "    Returns:\n",
    "      S: torch [W, C] scores\n",
    "      times: list[(start,end)]\n",
    "    \"\"\"\n",
    "    E, times = embed_windows(audio_path, seconds=seg_sec, hop_sec=hop_sec)\n",
    "    if E.numel() == 0 or P.numel() == 0:\n",
    "        return torch.empty(0), times\n",
    "    S = E @ P.T   # [W, D] @ [D, C] -> [W, C] cosine (already L2-normalized)\n",
    "    return S, times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4991bb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Turn per-window scores into time segments per class ---------------------\n",
    "def topk_overall(S, class_names, k=3):\n",
    "    \"\"\"Return overall top-k classes by mean score across windows.\"\"\"\n",
    "    if S.numel() == 0: return []\n",
    "    mean_scores = S.mean(dim=0).cpu().numpy()   # [C]\n",
    "    order = np.argsort(-mean_scores)[:k]\n",
    "    return [(class_names[i], float(mean_scores[i])) for i in order]\n",
    "\n",
    "def argmax_segments(S, times, class_names):\n",
    "    \"\"\"\n",
    "    Greedy segmentation by window argmax. Returns dict:\n",
    "      label -> list of (start_sec, end_sec)\n",
    "    \"\"\"\n",
    "    if S.numel() == 0: return {}\n",
    "    pred = S.argmax(dim=1).cpu().numpy()   # [W]\n",
    "    segs = {}\n",
    "    cur_lbl = None\n",
    "    cur_start = None\n",
    "    for i, cls in enumerate(pred):\n",
    "        lbl = class_names[int(cls)]\n",
    "        st, en = times[i]\n",
    "        if cur_lbl is None:\n",
    "            cur_lbl, cur_start = lbl, st\n",
    "        elif lbl != cur_lbl:\n",
    "            segs.setdefault(cur_lbl, []).append((cur_start, times[i-1][1]))\n",
    "            cur_lbl, cur_start = lbl, st\n",
    "    if cur_lbl is not None:\n",
    "        segs.setdefault(cur_lbl, []).append((cur_start, times[-1][1]))\n",
    "    return segs\n",
    "\n",
    "def threshold_segments(S, times, class_names, label, thresh, min_windows=1):\n",
    "    \"\"\"\n",
    "    Keep windows where score(label) >= thresh and merge consecutive.\n",
    "    Returns list of (start_sec, end_sec).\n",
    "    \"\"\"\n",
    "    if S.numel() == 0: return []\n",
    "    ci = class_names.index(label)\n",
    "    mask = (S[:, ci].cpu().numpy() >= thresh)\n",
    "    segs = []\n",
    "    i = 0\n",
    "    while i < len(mask):\n",
    "        if mask[i]:\n",
    "            start = times[i][0]\n",
    "            j = i\n",
    "            while j + 1 < len(mask) and mask[j+1]:\n",
    "                j += 1\n",
    "            end = times[j][1]\n",
    "            if (j - i + 1) >= min_windows:\n",
    "                segs.append((start, end))\n",
    "            i = j + 1\n",
    "        else:\n",
    "            i += 1\n",
    "    return segs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ac853c",
   "metadata": {},
   "source": [
    "# Instrument Detection\n",
    "\n",
    "This block runs the few-shot instrument recognizer on any audio file (e.g., a full song). We select a few instrument names that exist in our training data `(df_train)`.\n",
    "\n",
    "`build_prototypes_from_train()` creates a prototype vector for each instrument, an average embedding of 5 support examples (5-shot). These prototypes serve as our “reference sounds.”\n",
    "\n",
    "We load our own songs we want to analyze, preferably ones that contain any of the detect labels.\n",
    "\n",
    "The song is split into 10-second windows (with 0.5-second overlap).\n",
    "Each window is embedded with **AudioCLIP**, and we compute how similar it is to each prototype.\n",
    "\n",
    "The result `S` is a big matrix of similarity scores\n",
    "* → rows = time windows, columns = instruments.\n",
    "\n",
    "\n",
    "#### “Top-3 overall”\n",
    "This averages each instrument’s similarity score across *the entire song* and lists the top-3.\n",
    "So if `piano` = 0.98, it means **the overall sound of the song most closely resembles the piano prototype**.\n",
    "\n",
    "#### “Segments by argmax”\n",
    "For each 10-second window, we take the instrument with the **highest similarity** (the argmax).\n",
    "This gives a timeline of which instrument “dominates” at every point.\n",
    "The output shows:\n",
    "\n",
    "total number of seconds each instrument was dominant\n",
    "\n",
    "* a few example time spans where it led.\n",
    "* This is like a rough instrument activity map.\n",
    "\n",
    "#### “{focus} segments (thresholded)”\n",
    "This zooms in on one specific instrument (focus).\n",
    "\n",
    "It keeps only the time windows where the model’s confidence (similarity) for that instrument exceeds a chosen threshold (here 0.38).\n",
    "\n",
    "The result lists continuous time intervals where the model is confident the organ is playing.\n",
    "\n",
    "Changing `focus` lets you check any other instrument (e.g., \"piano\" or \"acoustic guitar\").\n",
    "\n",
    "##### Short summary\n",
    "\n",
    "* **Top-3 overall** → which instruments the song most sounds like overall.\n",
    "* **Segments by argmax** → who dominates over time.\n",
    "* **focus + thresholded** → when a chosen instrument is clearly present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bc9630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage ------------------------------------------------------------\n",
    "# 1) Choose which instruments we want to detect (must exist in training!)\n",
    "detect_labels = [\"acoustic guitar\", \"electric guitar\", \"piano\", \"organ\", \"clarinet\"]\n",
    "P, proto_labels, _ = build_prototypes_from_train(df_train, A_train,\n",
    "                                                 target_labels=detect_labels,\n",
    "                                                 k_support=5, seed=123)\n",
    "\n",
    "# 2) Path to your song (wav/mp3/flac; librosa will decode most)\n",
    "test_audio = \"audio/test_songs/clarinet.wav\"   # <-- change this\n",
    "\n",
    "# 3) Score windows and summarize\n",
    "S, times = score_file_against_prototypes(test_audio, P, proto_labels,\n",
    "                                         hop_sec=2.0, seg_sec=10)\n",
    "\n",
    "print(\"Top-3 overall:\")\n",
    "for name, sc in topk_overall(S, proto_labels, k=3):\n",
    "    print(f\"  {name:16s}  mean score={sc:.3f}\")\n",
    "\n",
    "# 4) Argmax segmentation (who dominates when)\n",
    "segs_argmax = argmax_segments(S, times, proto_labels)\n",
    "print(\"\\nSegments by argmax:\")\n",
    "for lbl, spans in segs_argmax.items():\n",
    "    total = sum(e - s for s, e in spans)\n",
    "    print(f\"  {lbl:16s}  total ~{total:5.1f}s  spans={[(round(s,1), round(e,1)) for s,e in spans[:5]]}...\")\n",
    "\n",
    "# 5) Focus on one instrument with a threshold\n",
    "focus = \"clarinet\"   # <-- change this\n",
    "segs_thr = threshold_segments(S, times, proto_labels, label=focus, thresh=0.85, min_windows=2)\n",
    "print(f\"\\n{focus} segments (thresholded):\")\n",
    "for (s, e) in segs_thr[:10]:\n",
    "    print(f\"  {s:6.2f}s → {e:6.2f}s  (~{e-s:.1f}s)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16de2e9",
   "metadata": {},
   "source": [
    "# My Results\n",
    "| Song                                        | What the model heard                                                | Interpretation                                                                                                        |\n",
    "| ------------------------------------------- | ------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Nuvole Bianche (Einaudi)**                | `piano` very strong (0.983), full-length detection                  | Correct, solo piano piece. Slight “acoustic guitar” similarity due to string-like overtones.                       |\n",
    "| **Ed Sheeran (Acoustic)**                   | Acoustic & electric guitars appear; also “organ/clarinet” confusion | Detects the guitar activity, but the vocal timbre tricked the model toward organ/clarinet (sustained mids).         |\n",
    "| **Red Hot Chili Peppers – Californication** | `electric guitar` and `organ` dominant across most of the track     | Matches song texture, guitar riff + background pad. Some overlap between these two due to harmonic sustain.        |\n",
    "| **Sing Sing Sing (Benny Goodman)**          | `clarinet` detected across long span (0–521 s)                      | Correct, clarinet lead is captured, though average scores biased toward guitar/piano because of full big-band mix. |\n",
    "| **Deep Purple – Child in Time**             | `electric guitar` and `organ` top scores (≈0.98/0.97)               | Perfect, main instruments of the song. Organ detected through most of track, just as expected.                     |\n",
    "\n",
    "#### Summary\n",
    "\n",
    "* The model successfully generalizes from clean IRMAS clips to real songs.\n",
    "* It often finds the right dominant instrument even in complex mixes.\n",
    "* Some confusion (e.g., organ ↔ piano, acoustic ↔ electric guitar) is natural since their timbres overlap.\n",
    "* Using the focus parameter, you can inspect when each instrument is active within a song.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650520d9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
