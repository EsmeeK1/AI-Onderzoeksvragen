{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: VAE\n",
    "**Beschrijving:** Het model werkt nu niet meer, omdat de data is veroudert, ik wil het model opnieuw trainen met nieuwe data om te kijken als het dan wel weer werkt.\n",
    "\n",
    "**Datum:** 27 Janurai 2025\n",
    "\n",
    "**Auteur:** Paul\n",
    "\n",
    "**Versie:** V0.2\n",
    "\n",
    "**Status:** Completed\n",
    "\n",
    "## Doel en Verwachtingen\n",
    "**Doelstelling:** Kijken als het mogelijk is om het probleem op te lossen.\n",
    "\n",
    "**Verwachting:** Ik hoop het\n",
    "\n",
    "## Data en Bronnen\n",
    "**Datasets:**\n",
    "- N.v.t. dataset is lokaal opgenomen\n",
    "\n",
    "**Externe Bronnen:**\n",
    "- N.v.t.\n",
    "\n",
    "## Software Configuratie\n",
    "**Software:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation and Analysis\n",
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import librosa\n",
    "\n",
    "# Machine Learning and Data Preprocessing\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Conv2D, Conv2DTranspose, Flatten, Reshape # type: ignore\n",
    "from tensorflow.keras.models import Model # type: ignore\n",
    "from tensorflow.keras.losses import mse # type: ignore\n",
    "import tensorflow.keras.backend as K # type: ignore\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "\n",
    "# System and File Management\n",
    "import sounddevice as sd # type: ignore\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Add a path to the scripts directory\n",
    "sys.path.append(os.path.abspath(os.path.join('../../', 'scripts')))\n",
    "\n",
    "# Project-Specific Modules\n",
    "from RetrievingData import load_dataframe, load_dataset # type: ignore\n",
    "from AnalysisPlots import plot_label_distribution # type: ignore\n",
    "from ProcessingData import process_audio # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "**Doel:** Een model dat werkt op basis van data dat als normaal wordt geschouwd in een specifieke context.\n",
    "\n",
    "**Succescriterium:** Een model dat werkt met normale data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "**Datasets:** Zelf opgenomen data van mijn kantoor thuis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>label</th>\n",
       "      <th>label_numerical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/kantoor_2025\\opname_20250127_110820.wav</td>\n",
       "      <td>kantoor_2025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/kantoor_2025\\opname_20250127_110822.wav</td>\n",
       "      <td>kantoor_2025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/kantoor_2025\\opname_20250127_110824.wav</td>\n",
       "      <td>kantoor_2025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/kantoor_2025\\opname_20250127_110826.wav</td>\n",
       "      <td>kantoor_2025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/kantoor_2025\\opname_20250127_110828.wav</td>\n",
       "      <td>kantoor_2025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>data/kantoor_2025\\opname_20250127_111319.wav</td>\n",
       "      <td>kantoor_2025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>data/kantoor_2025\\opname_20250127_111321.wav</td>\n",
       "      <td>kantoor_2025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>data/kantoor_2025\\opname_20250127_111323.wav</td>\n",
       "      <td>kantoor_2025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>data/kantoor_2025\\opname_20250127_111325.wav</td>\n",
       "      <td>kantoor_2025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>data/kantoor_2025\\opname_20250127_111327.wav</td>\n",
       "      <td>kantoor_2025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        file_path         label  \\\n",
       "0    data/kantoor_2025\\opname_20250127_110820.wav  kantoor_2025   \n",
       "1    data/kantoor_2025\\opname_20250127_110822.wav  kantoor_2025   \n",
       "2    data/kantoor_2025\\opname_20250127_110824.wav  kantoor_2025   \n",
       "3    data/kantoor_2025\\opname_20250127_110826.wav  kantoor_2025   \n",
       "4    data/kantoor_2025\\opname_20250127_110828.wav  kantoor_2025   \n",
       "..                                            ...           ...   \n",
       "145  data/kantoor_2025\\opname_20250127_111319.wav  kantoor_2025   \n",
       "146  data/kantoor_2025\\opname_20250127_111321.wav  kantoor_2025   \n",
       "147  data/kantoor_2025\\opname_20250127_111323.wav  kantoor_2025   \n",
       "148  data/kantoor_2025\\opname_20250127_111325.wav  kantoor_2025   \n",
       "149  data/kantoor_2025\\opname_20250127_111327.wav  kantoor_2025   \n",
       "\n",
       "     label_numerical  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  \n",
       "..               ...  \n",
       "145                0  \n",
       "146                0  \n",
       "147                0  \n",
       "148                0  \n",
       "149                0  \n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desired_labels = ['kantoor_2025']\n",
    "df, label_mapping = load_dataframe('data/', desired_labels)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "De Short-Time Fourier Transform (STFT) is een techniek om een audio- of tijdsignaal in het frequentiedomein te analyseren door het signaal op te splitsen in korte segmenten (vensters) over de tijd. Voor elk venster wordt een Fourier-transformatie uitgevoerd, wat de frequentie-inhoud van dat specifieke tijdssegment onthult. Dit biedt een manier om te analyseren welke frequenties aanwezig zijn op specifieke momenten in het signaal.\n",
    "\n",
    "In ons project gebruiken we de STFT om de frequentiecomponenten van het audiosignaal over de tijd te extraheren. We zetten de amplitude van de STFT-resultaten om in een decibel-schaal, waardoor we een spectrogram verkrijgen. Dit spectrogram geeft de amplitude van de frequenties weer over de tijd, wat nuttig is voor verdere verwerking in ons model (zoals bij de reconstructie van geluiden in een autoencoder).\n",
    "\n",
    "Waarom kiezen we voor STFT en een spectrogram in plaats van MFCC:\n",
    "\n",
    "1. **STFT en spectrogram bieden meer flexibiliteit en detail:**\n",
    "    - Een spectrogram is gebaseerd op de STFT en visualiseert de amplitude-informatie van het frequentiespectrum. Door de STFT te gebruiken en deze om te zetten naar een spectrogram, behouden we de volledige frequentie-informatie, zonder de fase, maar dit is vaak niet nodig voor onze toepassing. Dit biedt ons meer flexibiliteit en eenvoud in het verwerken van complexe audiogegevens.\n",
    "\n",
    "2. **Meer precieze frequentieanalyse:**\n",
    "    - In tegenstelling tot MFCC (Mel-Frequency Cepstral Coefficients), dat de frequentie-informatie samenvat op een manier die het menselijke gehoor nabootst (waarbij lagere frequenties gedetailleerder worden weergegeven dan hogere frequenties), biedt het spectrogram een fijnmaziger en directer overzicht van de volledige frequentieband. Dit is vooral nuttig voor anomaliedetectie, waarbij onverwachte frequenties op elk punt in het spectrum kunnen voorkomen.\n",
    "\n",
    "3. **Geen beperking tot menselijke perceptie:**\n",
    "    - MFCC is zeer geschikt voor spraakherkenning en taken gebaseerd op menselijke perceptie. Echter, voor onze toepassing in akoestische anomaliedetectie, willen we geen beperking leggen op de frequenties die we analyseren. Een spectrogram op basis van de STFT biedt een breed en gedetailleerd frequentiebereik, zonder zich te richten op alleen die frequenties die relevant zijn voor menselijke waarneming.\n",
    "\n",
    "4. **Aanpasbaarheid van parameters:**\n",
    "    - Met de STFT kunnen we de parameters zoals FFT-grootte en hop length dynamisch aanpassen, afhankelijk van de behoeften van ons project. Dit stelt ons in staat de tijd-resolutie en frequentie-resolutie te finetunen, wat belangrijk is voor de variatie in geluiden die we analyseren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProcessor:\n",
    "    def __init__(self, sr=44100, n_fft=1024, hop_length=512, target_length=128):\n",
    "        \"\"\"\n",
    "        Initialize the AudioProcessor class with the specified parameters for STFT and spectrogram processing.\n",
    "\n",
    "        Args:\n",
    "            sr (int): Sample rate for audio processing.\n",
    "            n_fft (int): Number of FFT components.\n",
    "            hop_length (int): Number of audio samples between STFT columns.\n",
    "            target_length (int): Target time dimension length for the spectrogram.\n",
    "        \"\"\"\n",
    "        self.sr = sr\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.target_length = target_length\n",
    "\n",
    "    def audio_to_specto(self, audio):\n",
    "        \"\"\"\n",
    "        Compute the Short-Time Fourier Transform (STFT) and return the normalized spectrogram.\n",
    "\n",
    "        Args:\n",
    "            audio (np.ndarray): Audio signal to process.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Normalized spectrogram with a channel dimension added.\n",
    "        \"\"\"\n",
    "        stft_result = librosa.stft(audio, n_fft=self.n_fft, hop_length=self.hop_length)\n",
    "        spectrogram = librosa.amplitude_to_db(abs(stft_result))\n",
    "        spectrogram_normalized = (spectrogram - np.min(spectrogram)) / (np.max(spectrogram) - np.min(spectrogram))\n",
    "\n",
    "        current_length = spectrogram_normalized.shape[1]\n",
    "        if current_length < self.target_length:\n",
    "            pad_width = self.target_length - current_length\n",
    "            spectrogram_normalized = np.pad(spectrogram_normalized, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            spectrogram_normalized = spectrogram_normalized[:, :self.target_length]\n",
    "\n",
    "        spectrogram_normalized = spectrogram_normalized[..., np.newaxis]  # Add channel dimension\n",
    "        return spectrogram_normalized\n",
    "\n",
    "    def process_df(self, file_path):\n",
    "        \"\"\"\n",
    "        Load an audio file, compute the STFT, and return the processed spectrogram.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the audio file.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Processed spectrogram.\n",
    "        \"\"\"\n",
    "        audio, _ = librosa.load(file_path, sr=self.sr)\n",
    "        return self.audio_to_specto(audio)\n",
    "\n",
    "    def process_audio_live(self, segment_samples):\n",
    "        \"\"\"\n",
    "        Process live audio samples and return the processed spectrogram in the shape (1, 513, 128, 1).\n",
    "\n",
    "        Args:\n",
    "            segment_samples (list): List of live audio samples.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Processed spectrogram with shape (1, 513, 128, 1).\n",
    "        \"\"\"\n",
    "        samples_np = np.array(segment_samples)\n",
    "        spectrogram = self.audio_to_specto(samples_np)\n",
    "        spectrogram = np.expand_dims(spectrogram, axis=0)  # Add batch dimension\n",
    "        return spectrogram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process audio files into spectrograms and resize them to a fixed length\n",
    "audio_processor = AudioProcessor()\n",
    "\n",
    "spectrograms = [\n",
    "    librosa.util.fix_length(audio_processor.process_df(row['file_path']), size=128, axis=1)\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "X_train = np.array(spectrograms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Een Variational Autoencoder (VAE) is een type autoencoder dat wordt gebruikt voor unsupervised learning, waarbij we een probabilistisch model leren dat complexe data zoals afbeeldingen, audio, of tekst kan reconstrueren. Het belangrijkste kenmerk van een VAE is dat het de latente ruimte waarin data worden geprojecteerd, dwingt om een continuüm te vormen, wat nuttig is voor taken zoals generatie van nieuwe data en anomaly detection.\n",
    "\n",
    "--- \n",
    "\n",
    "We beginnen met de sampling-functie, die een steekproef neemt uit de verdeling van de latente ruimte op basis van de `z_mean` en `z_log_var`. Dit zorgt ervoor dat het model probabilistisch is, wat betekent dat het in plaats van één vaste punt meerdere mogelijke punten in de latente ruimte kan genereren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling function for the latent space\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    \n",
    "    # epsilon is random noise drawn from a standard normal distribution\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    \n",
    "    # Take a sample based on z_mean and z_log_var\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `z_mean` en `z_log_var` zijn de parameters die de verdeling van de latente ruimte beschrijven.\n",
    "- `epsilon` introduceert willekeurigheid, wat het model in staat stelt om verschillende output te genereren vanuit dezelfde invoer.\n",
    "\n",
    "--- \n",
    "\n",
    "De **VAE**-class brengt de encoder en decoder samen en berekent de totale verliesfunctie (loss), bestaande uit de reconstructiefout en de KL-divergence. Dit zorgt ervoor dat het model leert hoe het de invoer kan reconstrueren en tegelijkertijd een georganiseerde latente ruimte leert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE custom class\n",
    "class VAE(Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Pass input through the encoder and obtain z_mean, z_log_var, and z (the sample)\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        \n",
    "        # Pass the sample through the decoder to get the reconstruction\n",
    "        reconstructed = self.decoder(z)\n",
    "\n",
    "        # Clip the log-variance to avoid numerical instability\n",
    "        z_log_var = K.clip(z_log_var, -10, 10)\n",
    "\n",
    "        # Compute the reconstruction loss using Mean Squared Error (MSE)\n",
    "        reconstruction_loss = mse(K.flatten(inputs), K.flatten(reconstructed))\n",
    "\n",
    "        # Compute KL divergence with clipped z_log_var\n",
    "        kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "\n",
    "        # Combine reconstruction loss and KL divergence to form the total loss\n",
    "        total_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "        # Add the total loss to the model\n",
    "        self.add_loss(total_loss)\n",
    "\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Encoder en decoder werken samen om de data te coderen naar een latente ruimte (`z_mean` en `z_log_var`) en deze daarna te reconstrueren.\n",
    "- Reconstructiefout meet hoe goed de gereconstrueerde invoer lijkt op de originele invoer (MSE).\n",
    "- KL-divergence dwingt de latente ruimte om dicht bij een normale verdeling te blijven.\n",
    "\n",
    "---\n",
    "\n",
    "De encoder neemt de invoer (spectrogram) en zet deze om naar een compactere representatie in de latente ruimte door middel van convolutionele lagen. Uiteindelijk berekent het `z_mean` en `z_log_var`, die de verdeling van de latente ruimte beschrijven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder definition\n",
    "input_shape = (513, 128, 1)  # Shape of the input (spectrogram)\n",
    "latent_dim = 8  # Dimension of the latent space\n",
    "\n",
    "# Input layer for spectrograms\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# Convolutional layers to extract features\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Fully connected layers that compute z_mean and z_log_var\n",
    "z_mean = Dense(latent_dim)(x)\n",
    "z_log_var = Dense(latent_dim)(x)\n",
    "\n",
    "# Sampling from the latent space\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# Build the encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name=\"encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conv2D-lagen extraheren patronen uit het spectrogram, zoals frequentie-inhoud en tijdsevolutie.\n",
    "- Flatten maakt de data geschikt voor de volledig verbonden lagen.\n",
    "- Dense-lagen berekenen de `z_mean` en `z_log_var`, die de latente ruimte beschrijven.\n",
    "\n",
    "--- \n",
    "\n",
    "De decoder neemt de steekproef (`z`) uit de latente ruimte en reconstrueert het originele spectrogram door middel van transposed convolutionele lagen, die werken als omgekeerde convoluties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder definition\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "\n",
    "# Reshape the latent space back into the original spectrogram shape\n",
    "x = Dense(513 * 128)(decoder_input)\n",
    "x = Reshape((513, 128, 1))(x)\n",
    "\n",
    "# Transposed convolutional layers to reconstruct the original spectrogram\n",
    "x = Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)\n",
    "outputs = Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "# Build the decoder model\n",
    "decoder = Model(decoder_input, outputs, name=\"decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Dense** zet de latente ruimte om naar de grootte van het oorspronkelijke spectrogram.\n",
    "- **Conv2DTranspose** werkt als een \"omgekeerde\" convolutie en bouwt het spectrogram laag voor laag opnieuw op.\n",
    "- **Sigmoid** in de output-laag normaliseert de waarden tussen 0 en 1, wat past bij het genormaliseerde inputformaat van het model.\n",
    "\n",
    "---\n",
    "\n",
    "Tot slot compileren we het model met de Adam-optimizer en trainen we het door de reconstructiefout en KL-divergence te minimaliseren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model (combining encoder and decoder)\n",
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "# Compile the model using the Adam optimizer\n",
    "vae.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Adam-optimizer** wordt gebruikt voor het trainen van het model, omdat het goed werkt bij niet-lineaire optimalisatieproblemen zoals deze.\n",
    "\n",
    "---\n",
    "\n",
    "In deze stap trainen we de Variational Autoencoder (**VAE**) op de normale audiogegevens. We geven zowel de invoer (`X_train`) als de gewenste output (`X_train`) als hetzelfde, omdat het doel van een autoencoder is om de invoer te reconstrueren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5/5 [==============================] - 6s 263ms/step - loss: 48116.6328\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 56389.2266\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 32691.9062\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 2.7579\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.0375\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.0221\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.0152\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.0180\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.0164\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.0148\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.0149\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.0150\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.0145\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.0145\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.0143\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.0139\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.0135\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.0129\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.0119\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.0110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2328deedc10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the autoencoder on normal audio\n",
    "vae.fit(X_train, X_train, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samengevat\n",
    "- De **Encoder** is verantwoordelijk voor het **comprimeren** van de oorspronkelijke spectogram naar de **latente  ruimte**. Deze latente ruimte is een lagere-dimensionale representatie (in dit geval bestaande uit 8 getallen) die de meest relevante informatie uit het spectrogram samenvat. De encoder zorgt ervoor dat het model leert wat de \"essentie\" is van het geluid zonder alle details vast te hoeven leggen. \n",
    "\n",
    "- De **Decoder** is verantwoordelijk voor het reconstrueren van het spectrogram vanuit de latente ruimte. Dit betekent dat het model probeert de oorspronkelijke input (het spectrogram) terug te krijgen, gebaseerd op die 8 getallen die uit de latente ruimte komen. De decoder gebruikt deze getallen als \"startpunt\" en bouwt laag voor laag het volledige spectrogram opnieuw op.\n",
    "\n",
    "- De **loss-functie** bestaat uit twee delen: **reconstructiefout (MSE)** en **KL-divergence**. Samen zorgen deze voor de optimale balans tussen het nauwkeurig reconstrueren van de inputdata en het organiseren van de latente ruimte.\n",
    "    - **Reconstructiefout (MSE)** meet hoe goed het model de oorspronkelijke input (het spectrogram) kan reconstrueren na compressie in de latente ruimte. Het vergelijkt het gereconstrueerde spectrogram met het originele spectrogram en berekent het kwadratische verschil tussen de twee. Een lagere reconstructiefout betekent dat het model goed in staat is om de belangrijkste informatie van het geluid vast te leggen en nauwkeurig te reconstrueren.\n",
    "    - **KL-divergence** meet hoe goed de verdeling van de latente ruimte overeenkomt met een standaard normale verdeling. Het dwingt het model om de latente ruimte te structureren, zodat het een continuüm van mogelijke representaties vormt. Dit helpt het model bij het genereren van nieuwe data en zorgt ervoor dat de latente ruimte geordend blijft, wat belangrijk is voor de generalisatie van het model.\n",
    "    - Samen zorgen **MSE** en **KL-divergence** ervoor dat het model zowel een nauwkeurige reconstructie kan maken als een gestructureerde latente ruimte behoudt. Het doel tijdens training is om de som van deze twee termen, de totale loss, te minimaliseren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Het grootste probleem met deze aanpak is de evaluation van het model. Er zijn geen labels om te concluderen wat goed en fout is. Daarom is er voor gekozen om te kijken naar verschillende thresholds om vervolgens hier live meet te testen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 30ms/step\n",
      "Threshold for anomaly detection: 0.020535769872367382\n",
      "Mean reconstruction error on training data: 0.010228280909359455\n",
      "Minimal reconstruction error on training data: 0.006618628278374672\n",
      "Maximum reconstruction error on training data: 0.04694449156522751\n"
     ]
    }
   ],
   "source": [
    "# Reconstruct the training data (normal sounds) using the trained VAE\n",
    "X_train_reconstructed = vae.predict(X_train)  # X_train contains the normal sounds used for training\n",
    "\n",
    "# Calculate the Mean Squared Error (MSE) for the training data (normal sounds)\n",
    "# MSE is calculated per sample by taking the mean squared difference between original and reconstructed data\n",
    "mse_train = np.mean(np.power(X_train - X_train_reconstructed, 2), axis=(1, 2, 3))  # MSE for the normal data\n",
    "\n",
    "# Set a threshold for anomaly detection based on the distribution of training reconstruction errors\n",
    "# Here, the threshold is set to the mean of the MSE plus 2 standard deviations\n",
    "threshold = np.mean(mse_train) + 2 * np.std(mse_train)  # Experiment with different threshold values if needed\n",
    "\n",
    "# Output the chosen threshold value for anomaly detection\n",
    "print(f\"Threshold for anomaly detection: {threshold}\")\n",
    "\n",
    "# Output the distribution of reconstruction errors for the training data\n",
    "print(f\"Mean reconstruction error on training data: {np.mean(mse_train)}\")\n",
    "print(f\"Minimal reconstruction error on training data: {np.min(mse_train)}\")\n",
    "print(f\"Maximum reconstruction error on training data: {np.max(mse_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters as used during live predection\n",
    "sample_rate = 44100  # Sampling rate used to capture audio data, typically in samples per second\n",
    "segment_duration = 2  # Duration of each audio segment in seconds (each audio fragment is 2 seconds long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare global segment_samples before any processing\n",
    "segment_samples = []\n",
    "\n",
    "def audio_callback(indata, frames, time_info, status, audio_processor):\n",
    "    global segment_samples  # Reference the global segment_samples\n",
    "\n",
    "    if status:\n",
    "        print(status)\n",
    "    segment_samples.extend(indata[:, 0])  # Extend the list with new audio data\n",
    "\n",
    "    if len(segment_samples) >= sample_rate * segment_duration:  # Check if enough audio has been collected\n",
    "        current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "        print(f\"Prediction uitgevoerd op: {current_time}\")\n",
    "\n",
    "        # Process the audio into spectrogram features using the AudioProcessor\n",
    "        audio_features = audio_processor.process_audio_live(segment_samples)\n",
    "\n",
    "        # Check the shape of the live input (should be (1, 513, 128, 1))\n",
    "        print(f\"Shape of live input: {audio_features.shape}\")\n",
    "\n",
    "        # Predict anomalies using the VAE model\n",
    "        reconstructed_audio = vae.predict(audio_features)\n",
    "\n",
    "        # Calculate the reconstruction error (Mean Squared Error)\n",
    "        mse = np.mean(np.power(audio_features - reconstructed_audio, 2))\n",
    "\n",
    "        # Check for anomaly\n",
    "        if mse > threshold:\n",
    "            print(f\"Anomalie gedetecteerd! Reconstructiefout: {mse}\")\n",
    "        else:\n",
    "            print(f\"Normaal geluid. Reconstructiefout: {mse}\")\n",
    "\n",
    "        # Reset the samples for the next batch\n",
    "        segment_samples = []  # Reset the global segment_samples for the next batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start streaming van audio...\n",
      "Prediction uitgevoerd op: 2025-02-03 10:57:16\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 231ms/step\n",
      "Normaal geluid. Reconstructiefout: 0.007363243028521538\n",
      "input overflow\n",
      "Prediction uitgevoerd op: 2025-02-03 10:57:18\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Normaal geluid. Reconstructiefout: 0.007461633533239365\n",
      "Prediction uitgevoerd op: 2025-02-03 10:57:20\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Normaal geluid. Reconstructiefout: 0.007335428148508072\n",
      "Prediction uitgevoerd op: 2025-02-03 10:57:22\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Normaal geluid. Reconstructiefout: 0.018336864188313484\n",
      "Prediction uitgevoerd op: 2025-02-03 10:57:24\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Normaal geluid. Reconstructiefout: 0.013046624138951302\n",
      "input overflow\n",
      "Prediction uitgevoerd op: 2025-02-03 10:57:26\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Normaal geluid. Reconstructiefout: 0.007244473323225975\n",
      "Prediction uitgevoerd op: 2025-02-03 10:57:28\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Normaal geluid. Reconstructiefout: 0.007730718702077866\n",
      "Prediction uitgevoerd op: 2025-02-03 10:57:30\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Normaal geluid. Reconstructiefout: 0.00836800504475832\n",
      "Prediction uitgevoerd op: 2025-02-03 10:57:32\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Normaal geluid. Reconstructiefout: 0.00763415964320302\n",
      "Prediction uitgevoerd op: 2025-02-03 10:57:34\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Normaal geluid. Reconstructiefout: 0.007706765551120043\n",
      "input overflow\n",
      "Prediction uitgevoerd op: 2025-02-03 10:57:37\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Anomalie gedetecteerd! Reconstructiefout: 0.0600866861641407\n",
      "Prediction uitgevoerd op: 2025-02-03 10:57:39\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Normaal geluid. Reconstructiefout: 0.007538237143307924\n",
      "Prediction uitgevoerd op: 2025-02-03 10:57:41\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Normaal geluid. Reconstructiefout: 0.01065627858042717\n",
      "input overflow\n",
      "Prediction uitgevoerd op: 2025-02-03 10:57:43\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Anomalie gedetecteerd! Reconstructiefout: 0.06492438167333603\n",
      "Prediction uitgevoerd op: 2025-02-03 10:57:45\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Normaal geluid. Reconstructiefout: 0.007915190421044827\n",
      "Prediction uitgevoerd op: 2025-02-03 10:57:47\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Normaal geluid. Reconstructiefout: 0.0076005649752914906\n",
      "Prediction uitgevoerd op: 2025-02-03 10:57:49\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Normaal geluid. Reconstructiefout: 0.007484588772058487\n",
      "Prediction uitgevoerd op: 2025-02-03 10:57:51\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Normaal geluid. Reconstructiefout: 0.00828776229172945\n",
      "Prediction uitgevoerd op: 2025-02-03 10:57:53\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Anomalie gedetecteerd! Reconstructiefout: 0.07573066651821136\n",
      "Prediction uitgevoerd op: 2025-02-03 10:57:55\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Anomalie gedetecteerd! Reconstructiefout: 0.0735984817147255\n",
      "Prediction uitgevoerd op: 2025-02-03 10:57:57\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Normaal geluid. Reconstructiefout: 0.007526910398155451\n",
      "Prediction uitgevoerd op: 2025-02-03 10:57:59\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Normaal geluid. Reconstructiefout: 0.008244101889431477\n",
      "Prediction uitgevoerd op: 2025-02-03 10:58:01\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Normaal geluid. Reconstructiefout: 0.007713239174336195\n",
      "Prediction uitgevoerd op: 2025-02-03 10:58:03\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Normaal geluid. Reconstructiefout: 0.009715840220451355\n",
      "Prediction uitgevoerd op: 2025-02-03 10:58:05\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Normaal geluid. Reconstructiefout: 0.007365770637989044\n",
      "Prediction uitgevoerd op: 2025-02-03 10:58:07\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Normaal geluid. Reconstructiefout: 0.006991142872720957\n",
      "Prediction uitgevoerd op: 2025-02-03 10:58:09\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Normaal geluid. Reconstructiefout: 0.00798090174794197\n",
      "Prediction uitgevoerd op: 2025-02-03 10:58:11\n",
      "Shape of live input: (1, 513, 128, 1)\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Audio streaming gestopt.\n",
      "Normaal geluid. Reconstructiefout: 0.00779777392745018\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize the AudioProcessor object\n",
    "    audio_processor = AudioProcessor()\n",
    "\n",
    "    print(\"Start streaming van audio...\")\n",
    "    with sd.InputStream(callback=lambda indata, frames, time_info, status: audio_callback(indata, frames, time_info, status, audio_processor),\n",
    "                        channels=1, samplerate=sample_rate):\n",
    "        try:\n",
    "            while True:\n",
    "                sd.sleep(100)  # Keep the process active while audio is streamed\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Audio streaming gestopt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Paul\\AppData\\Local\\Temp\\tmpxne45mx3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Paul\\AppData\\Local\\Temp\\tmpxne45mx3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model succesvol opgeslagen als vae_model.tflite\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Direct converteren naar TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(vae)  # Gebruik je VAE-model direct\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Opslaan als .tflite-bestand\n",
    "with open(\"models/vae_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Model succesvol opgeslagen als vae_model.tflite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dit zegt natuurlijk niks over hoe goed het model werkt, want het is live testing. Maar deze aanpak is redelijk succesvol. Wanneer ik in mijn handen klap of tegen de microfoon tik dan ziet hij deze als een abnormaliteit.\n",
    "\n",
    "---\n",
    "\n",
    "## Deployment\n",
    "**Conclusie:**\n",
    "- De huidige aanpak lijkt hoopvol en deze moeten we testen op de Raspberry Pi \n",
    "\n",
    "**Volgende Stappen:**\n",
    "- Data Augmentatie toepassen\n",
    "- Beter idee krijgen hoe de data eruit moet zien voor dit model\n",
    "- Kijken welke aanpassesn een positief gevolg heeft voor het resultaat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
