{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710c38a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "\n",
    "import librosa\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f73e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = Path(\"../../data/processed/icbhi/icbhi_segments.csv\")\n",
    "WAV_DIR  = Path(\"../../data/processed/icbhi/audio_4000hz_bp_segments\")\n",
    "\n",
    "OUT_DIR = Path(\"../../data/processed/icbhi/baseline_runs_all_labels\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8656152",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "LABEL_COL   = \"acoustic_label\"\n",
    "PATIENT_COL = \"patient_id\"\n",
    "\n",
    "df[LABEL_COL] = df[LABEL_COL].astype(str).str.strip().str.lower()\n",
    "\n",
    "EXPECTED = {\"normal\", \"crackle\", \"wheeze\", \"mixed\", \"reject\"}\n",
    "bad_labels = set(df[LABEL_COL].unique()) - EXPECTED\n",
    "if bad_labels:\n",
    "    raise ValueError(f\"Unexpected labels: {sorted(bad_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d756154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d442bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['acoustic_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ff1eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop reject labels\n",
    "df = df[df[LABEL_COL] != \"reject\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7f728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop mixed labels\n",
    "df = df[df[LABEL_COL] != \"mixed\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e65bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEGMENT_NAME_PATTERN = \"{stem}_seg{seg:03d}.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d7c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_segment_path(file_name: str, segment_index: int) -> Path:\n",
    "    stem = str(file_name).strip()\n",
    "    if stem.lower().endswith(\".wav\"):\n",
    "        stem = stem[:-4]\n",
    "    fn = SEGMENT_NAME_PATTERN.format(stem=stem, seg=int(segment_index))\n",
    "    return (WAV_DIR / fn).resolve()\n",
    "\n",
    "df[\"audio_path\"] = df.apply(lambda r: build_segment_path(r[\"file_name\"], r[\"segment_index\"]), axis=1)\n",
    "df = df[df[\"audio_path\"].apply(lambda p: p.exists())].reset_index(drop=True)\n",
    "\n",
    "p0 = df.loc[0, \"audio_path\"]\n",
    "info = sf.info(str(p0))\n",
    "if info.samplerate != 4000:\n",
    "    raise ValueError(f\"Unexpected samplerate {info.samplerate} in {p0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f479d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df[PATIENT_COL].astype(str)\n",
    "\n",
    "gss1 = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=SEED)\n",
    "train_idx, test_idx = next(gss1.split(df, groups=groups))\n",
    "df_trainval = df.iloc[train_idx].reset_index(drop=True)\n",
    "df_test = df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "groups_tv = df_trainval[PATIENT_COL].astype(str)\n",
    "gss2 = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=SEED)\n",
    "train_idx2, val_idx2 = next(gss2.split(df_trainval, groups=groups_tv))\n",
    "df_train = df_trainval.iloc[train_idx2].reset_index(drop=True)\n",
    "df_val = df_trainval.iloc[val_idx2].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676d00c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[LABEL_COL].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e9127",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [\"normal\", \"crackle\", \"wheeze\"]\n",
    "label2id = {l: i for i, l in enumerate(LABELS)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6fcf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(df_train: pd.DataFrame) -> torch.Tensor:\n",
    "    counts = df_train[LABEL_COL].value_counts().reindex(LABELS, fill_value=0).values.astype(np.float32)\n",
    "    weights = counts.sum() / (len(LABELS) * np.maximum(counts, 1.0))\n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "def make_balanced_subset(df_train: pd.DataFrame, normal_name=\"normal\") -> pd.DataFrame:\n",
    "    vc = df_train[LABEL_COL].value_counts()\n",
    "    if normal_name not in vc.index:\n",
    "        return df_train\n",
    "    non_normal = vc.drop(index=[normal_name])\n",
    "    if len(non_normal) == 0:\n",
    "        return df_train\n",
    "    target = int(non_normal.min())\n",
    "    df_normal = df_train[df_train[LABEL_COL] == normal_name].sample(\n",
    "        n=min(target, int(vc[normal_name])),\n",
    "        random_state=SEED\n",
    "    )\n",
    "    df_rest = df_train[df_train[LABEL_COL] != normal_name]\n",
    "    return pd.concat([df_rest, df_normal], ignore_index=True).sample(frac=1, random_state=SEED).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bdea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "FS_TARGET = 4000\n",
    "N_FFT = 256\n",
    "HOP_LENGTH = 64\n",
    "N_MELS = 64\n",
    "FMIN = 20\n",
    "FMAX = FS_TARGET // 2\n",
    "\n",
    "CACHE_DIR = OUT_DIR / \"features_cache\"\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aade09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax01(x: np.ndarray) -> np.ndarray:\n",
    "    x_min = float(np.min(x))\n",
    "    x_max = float(np.max(x))\n",
    "    if (x_max - x_min) < 1e-12:\n",
    "        return np.zeros_like(x, dtype=np.float32)\n",
    "    return ((x - x_min) / (x_max - x_min)).astype(np.float32)\n",
    "\n",
    "def feat_logmel(y: np.ndarray, sr: int) -> np.ndarray:\n",
    "    S = librosa.feature.melspectrogram(\n",
    "        y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH,\n",
    "        n_mels=N_MELS, fmin=FMIN, fmax=FMAX, power=2.0\n",
    "    )\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "    return minmax01(S_db)\n",
    "\n",
    "def feat_stft(y: np.ndarray, sr: int) -> np.ndarray:\n",
    "    D = librosa.stft(y=y, n_fft=N_FFT, hop_length=HOP_LENGTH, center=True)\n",
    "    P = (np.abs(D) ** 2).astype(np.float32)\n",
    "    P_db = librosa.power_to_db(P, ref=np.max)\n",
    "    return minmax01(P_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bed1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_key(row) -> str:\n",
    "    stem = str(row[\"file_name\"]).replace(\".wav\", \"\")\n",
    "    seg = int(row[\"segment_index\"])\n",
    "    return f\"{stem}__seg_{seg:03d}\"\n",
    "\n",
    "def cache_path(feature_type: str, key: str) -> Path:\n",
    "    d = CACHE_DIR / feature_type\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    return d / f\"{key}.npz\"\n",
    "\n",
    "def extract_and_cache(row, feature_type: str) -> Path:\n",
    "    key = segment_key(row)\n",
    "    cp = cache_path(feature_type, key)\n",
    "    if cp.exists():\n",
    "        return cp\n",
    "\n",
    "    audio_path = Path(row[\"audio_path\"])\n",
    "    if not audio_path.exists():\n",
    "        raise FileNotFoundError(str(audio_path))\n",
    "\n",
    "    y, sr = sf.read(str(audio_path), dtype=\"float32\")\n",
    "    if y.ndim > 1:\n",
    "        y = np.mean(y, axis=1)\n",
    "\n",
    "    if sr != FS_TARGET:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=FS_TARGET)\n",
    "        sr = FS_TARGET\n",
    "\n",
    "    if feature_type == \"logmel\":\n",
    "        X = feat_logmel(y, sr)\n",
    "    elif feature_type == \"stft\":\n",
    "        X = feat_stft(y, sr)\n",
    "    else:\n",
    "        raise ValueError(feature_type)\n",
    "\n",
    "    X = X[..., None].astype(np.float32)\n",
    "    y_id = label2id[str(row[LABEL_COL]).strip().lower()]\n",
    "\n",
    "    np.savez_compressed(cp, X=X, y=y_id)\n",
    "    return cp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebff77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedFeatureDataset(Dataset):\n",
    "    def __init__(self, df_split: pd.DataFrame, feature_type: str):\n",
    "        self.df = df_split.reset_index(drop=True).copy()\n",
    "        self.feature_type = feature_type\n",
    "        self.cache_files = []\n",
    "        for i in range(len(self.df)):\n",
    "            row = self.df.iloc[i]\n",
    "            cp = extract_and_cache(row, feature_type)\n",
    "            self.cache_files.append(str(cp))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cache_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        z = np.load(self.cache_files[idx])\n",
    "        X = z[\"X\"]\n",
    "        y = int(z[\"y\"])\n",
    "        X = torch.from_numpy(X).permute(2, 0, 1).contiguous()\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a18caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, n_classes: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=(4, 2), padding=(1, 0)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "\n",
    "            nn.Conv2d(8, 32, kernel_size=(4, 2), padding=(1, 0)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=(4, 2), padding=(1, 0)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=(4, 2), padding=(1, 0)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(1, 2)),  # reduced stride in height dim\n",
    "        )\n",
    "\n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        self.fc1 = nn.Linear(1, 2000)  # placeholder; we set in forward after we know flatten dim\n",
    "        self.dropout2 = nn.Dropout(p=0.50)\n",
    "        self.out = nn.Linear(2000, n_classes)\n",
    "\n",
    "        self._fc_initialized = False\n",
    "\n",
    "    def _init_fc(self, x):\n",
    "        flat_dim = x.shape[1]\n",
    "        self.fc1 = nn.Linear(flat_dim, 2000).to(x.device)\n",
    "        self._fc_initialized = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        if not self._fc_initialized:\n",
    "            self._init_fc(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        return self.out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2b1b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, loader, optimizer, criterion, train=True):\n",
    "    model.train(train)\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X, y in loader:\n",
    "        X = X.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_loss += float(loss.item()) * X.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += int((pred == y).sum().item())\n",
    "        total += int(X.size(0))\n",
    "    return total_loss / max(total, 1), correct / max(total, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, loader):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    for X, y in loader:\n",
    "        X = X.to(DEVICE)\n",
    "        logits = model(X)\n",
    "        pred = logits.argmax(dim=1).cpu().numpy()\n",
    "        ys.extend(y.numpy().tolist())\n",
    "        ps.extend(pred.tolist())\n",
    "    return np.array(ys), np.array(ps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bd83c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(feature_type=\"logmel\", use_weights=False, use_balanced=False,\n",
    "                   epochs=50, batch_size=8, lr=1e-3):\n",
    "    exp_name = f\"{feature_type}__weights_{int(use_weights)}__balanced_{int(use_balanced)}\"\n",
    "    exp_dir = OUT_DIR / exp_name\n",
    "    exp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_df = make_balanced_subset(df_train) if use_balanced else df_train\n",
    "\n",
    "    ds_train = CachedFeatureDataset(train_df, feature_type)\n",
    "    ds_val   = CachedFeatureDataset(df_val, feature_type)\n",
    "    ds_test  = CachedFeatureDataset(df_test, feature_type)\n",
    "\n",
    "    dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    dl_val   = DataLoader(ds_val, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    dl_test  = DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    model = SmallCNN(n_classes=len(LABELS)).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    if use_weights:\n",
    "        w = compute_class_weights(train_df).to(DEVICE)\n",
    "        criterion = nn.CrossEntropyLoss(weight=w)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    cfg = {\n",
    "        \"feature_type\": feature_type,\n",
    "        \"use_weights\": use_weights,\n",
    "        \"use_balanced\": use_balanced,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"lr\": lr,\n",
    "        \"seed\": SEED,\n",
    "        \"labels\": LABELS,\n",
    "        \"csv_path\": str(CSV_PATH),\n",
    "        \"wav_dir\": str(WAV_DIR),\n",
    "        \"segment_name_pattern\": SEGMENT_NAME_PATTERN,\n",
    "    }\n",
    "    (exp_dir / \"config.json\").write_text(json.dumps(cfg, indent=2))\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_path = exp_dir / \"best.pt\"\n",
    "\n",
    "    history = []\n",
    "    for ep in range(1, epochs + 1):\n",
    "        tr_loss, tr_acc = run_epoch(model, dl_train, optimizer, criterion, train=True)\n",
    "        va_loss, va_acc = run_epoch(model, dl_val, optimizer, criterion, train=False)\n",
    "        history.append({\n",
    "            \"epoch\": ep,\n",
    "            \"train_loss\": tr_loss, \"train_acc\": tr_acc,\n",
    "            \"val_loss\": va_loss, \"val_acc\": va_acc\n",
    "        })\n",
    "        if va_loss < best_val:\n",
    "            best_val = va_loss\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "\n",
    "    pd.DataFrame(history).to_csv(exp_dir / \"history.csv\", index=False)\n",
    "\n",
    "    model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "    y_true, y_pred = predict(model, dl_test)\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=LABELS, digits=4, output_dict=True)\n",
    "    pd.DataFrame(report).to_csv(exp_dir / \"test_classification_report.csv\")\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    np.save(exp_dir / \"confusion_matrix.npy\", cm)\n",
    "\n",
    "    del model, optimizer, criterion, dl_train, dl_val, dl_test, ds_train, ds_val, ds_test\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return exp_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d33c731",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = []\n",
    "for feat in [\"logmel\", \"stft\"]:\n",
    "    for w in [False, True]:\n",
    "        for b in [False, True]:\n",
    "            runs.append(run_experiment(feature_type=feat, use_weights=w, use_balanced=b, epochs=50))\n",
    "runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268ce8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = []\n",
    "\n",
    "for d in runs:\n",
    "    rep_path = Path(d) / \"test_classification_report.csv\"\n",
    "    rep = pd.read_csv(rep_path, index_col=0)\n",
    "\n",
    "    if \"f1-score\" in rep.columns:\n",
    "        macro_f1 = float(rep.loc[\"macro avg\", \"f1-score\"])\n",
    "    elif \"f1-score\" in rep.index:\n",
    "        macro_f1 = float(rep.loc[\"f1-score\", \"macro avg\"])\n",
    "    else:\n",
    "        # last resort: try transpose\n",
    "        rep_t = rep.T\n",
    "        if \"f1-score\" in rep_t.columns:\n",
    "            macro_f1 = float(rep_t.loc[\"macro avg\", \"f1-score\"])\n",
    "        else:\n",
    "            raise KeyError(f\"Could not find macro F1 in {rep_path}\")\n",
    "\n",
    "    summary.append({\"run\": Path(d).name, \"macro_f1\": macro_f1})\n",
    "\n",
    "summary_df = (\n",
    "    pd.DataFrame(summary)\n",
    "      .sort_values(\"macro_f1\", ascending=False)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ec48f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def parse_exp_name(exp_dir: Path) -> str:\n",
    "    name = Path(exp_dir).name\n",
    "    parts = name.split(\"__\")\n",
    "    kv = {}\n",
    "    for p in parts:\n",
    "        if \"_weights_\" in p:\n",
    "            # e.g. logmel__weights_1__balanced_0 (but your pattern is feature__weights_x__balanced_y)\n",
    "            pass\n",
    "    # robust parse: feature_type is first token, then weights_x, balanced_y\n",
    "    feature_type = parts[0] if len(parts) > 0 else name\n",
    "    weights = None\n",
    "    balanced = None\n",
    "    for p in parts[1:]:\n",
    "        if p.startswith(\"weights_\"):\n",
    "            weights = p.split(\"weights_\")[-1]\n",
    "        if p.startswith(\"balanced_\"):\n",
    "            balanced = p.split(\"balanced_\")[-1]\n",
    "\n",
    "    def yn(v):\n",
    "        if v is None:\n",
    "            return \"Unknown\"\n",
    "        return \"Yes\" if str(v) in {\"1\", \"true\", \"True\", \"yes\", \"Yes\"} else \"No\"\n",
    "\n",
    "    title = f\"ICBHI Baseline | Features: {feature_type.upper()} | Class Weights: {yn(weights)} | Balanced Subset: {yn(balanced)}\"\n",
    "    return title\n",
    "\n",
    "def load_report_csv(report_path: Path) -> pd.DataFrame:\n",
    "    rep = pd.read_csv(report_path, index_col=0)\n",
    "    # Ensure rows are classes/avg and columns are metrics\n",
    "    # If currently metrics are rows and classes are columns -> transpose\n",
    "    if \"f1-score\" not in rep.columns and \"f1-score\" in rep.index:\n",
    "        rep = rep.T\n",
    "    return rep\n",
    "\n",
    "def plot_train_curves(exp_dir, save=True, dpi=300):\n",
    "    exp_dir = Path(exp_dir)\n",
    "    title = parse_exp_name(exp_dir)\n",
    "\n",
    "    hist_path = exp_dir / \"history.csv\"\n",
    "    if not hist_path.exists():\n",
    "        raise FileNotFoundError(hist_path)\n",
    "\n",
    "    hist = pd.read_csv(hist_path)\n",
    "\n",
    "    fig = plt.figure(figsize=(11, 4.5), dpi=dpi)\n",
    "    fig.suptitle(title, fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax1.plot(hist[\"epoch\"], hist[\"train_acc\"], label=\"Train Acc\", linewidth=2)\n",
    "    ax1.plot(hist[\"epoch\"], hist[\"val_acc\"], label=\"Val Acc\", linewidth=2)\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax1.set_ylim(0, 1.0)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend(frameon=False)\n",
    "\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    ax2.plot(hist[\"epoch\"], hist[\"train_loss\"], label=\"Train Loss\", linewidth=2)\n",
    "    ax2.plot(hist[\"epoch\"], hist[\"val_loss\"], label=\"Val Loss\", linewidth=2)\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend(frameon=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save:\n",
    "        out = exp_dir / \"plot_curves_acc_loss.png\"\n",
    "        fig.savefig(out, bbox_inches=\"tight\")\n",
    "    return fig\n",
    "\n",
    "def plot_confusion_matrix(exp_dir, labels, normalize=None, save=True, dpi=300):\n",
    "    \"\"\"\n",
    "    normalize:\n",
    "      None -> raw counts\n",
    "      \"true\" -> rows sum to 1 (recall-normalized)\n",
    "      \"pred\" -> cols sum to 1 (precision-normalized)\n",
    "    \"\"\"\n",
    "    exp_dir = Path(exp_dir)\n",
    "    title = parse_exp_name(exp_dir)\n",
    "\n",
    "    cm_path = exp_dir / \"confusion_matrix.npy\"\n",
    "    if not cm_path.exists():\n",
    "        raise FileNotFoundError(cm_path)\n",
    "\n",
    "    cm = np.load(cm_path).astype(float)\n",
    "\n",
    "    if normalize == \"true\":\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        cm_disp = np.divide(cm, row_sums, out=np.zeros_like(cm), where=row_sums != 0)\n",
    "        subtitle = \"Confusion Matrix (Normalized by True Class)\"\n",
    "        fmt = \".2f\"\n",
    "    elif normalize == \"pred\":\n",
    "        col_sums = cm.sum(axis=0, keepdims=True)\n",
    "        cm_disp = np.divide(cm, col_sums, out=np.zeros_like(cm), where=col_sums != 0)\n",
    "        subtitle = \"Confusion Matrix (Normalized by Predicted Class)\"\n",
    "        fmt = \".2f\"\n",
    "    else:\n",
    "        cm_disp = cm\n",
    "        subtitle = \"Confusion Matrix (Counts)\"\n",
    "        fmt = \".0f\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 6), dpi=dpi)\n",
    "    ax.set_title(f\"{title}\\n{subtitle}\", fontsize=13, fontweight=\"bold\", pad=12)\n",
    "\n",
    "    im = ax.imshow(cm_disp, interpolation=\"nearest\")\n",
    "    cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "    ax.set_xticks(np.arange(len(labels)))\n",
    "    ax.set_yticks(np.arange(len(labels)))\n",
    "    ax.set_xticklabels([l.upper() for l in labels], rotation=30, ha=\"right\")\n",
    "    ax.set_yticklabels([l.upper() for l in labels])\n",
    "\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "\n",
    "    # annotate\n",
    "    thresh = (cm_disp.max() + cm_disp.min()) / 2.0 if cm_disp.size else 0.0\n",
    "    for i in range(cm_disp.shape[0]):\n",
    "        for j in range(cm_disp.shape[1]):\n",
    "            val = cm_disp[i, j]\n",
    "            ax.text(\n",
    "                j, i, format(val, fmt),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                fontsize=10,\n",
    "                color=\"white\" if val > thresh else \"black\"\n",
    "            )\n",
    "\n",
    "    ax.set_xlim(-0.5, len(labels) - 0.5)\n",
    "    ax.set_ylim(len(labels) - 0.5, -0.5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save:\n",
    "        out = exp_dir / f\"plot_confusion_matrix_{normalize or 'counts'}.png\"\n",
    "        fig.savefig(out, bbox_inches=\"tight\")\n",
    "    return fig\n",
    "\n",
    "def plot_classification_report(exp_dir, save=True, dpi=300):\n",
    "    exp_dir = Path(exp_dir)\n",
    "    title = parse_exp_name(exp_dir)\n",
    "\n",
    "    rep_path = exp_dir / \"test_classification_report.csv\"\n",
    "    if not rep_path.exists():\n",
    "        raise FileNotFoundError(rep_path)\n",
    "\n",
    "    rep = load_report_csv(rep_path)\n",
    "\n",
    "    # Keep only core metrics; align rows in a nice order if present\n",
    "    cols = [c for c in [\"precision\", \"recall\", \"f1-score\", \"support\"] if c in rep.columns]\n",
    "    rep = rep[cols].copy()\n",
    "\n",
    "    row_order = []\n",
    "    for r in [\"normal\", \"crackle\", \"wheeze\", \"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "        if r in rep.index:\n",
    "            row_order.append(r)\n",
    "    # also include any other rows not covered\n",
    "    for r in rep.index:\n",
    "        if r not in row_order:\n",
    "            row_order.append(r)\n",
    "    rep = rep.loc[row_order]\n",
    "\n",
    "    # Make it figure-friendly: round metrics\n",
    "    rep_disp = rep.copy()\n",
    "    for c in rep_disp.columns:\n",
    "        if c == \"support\":\n",
    "            rep_disp[c] = rep_disp[c].astype(float).round(0).astype(int)\n",
    "        elif c == \"accuracy\":\n",
    "            rep_disp[c] = rep_disp[c].astype(float).round(4)\n",
    "        else:\n",
    "            rep_disp[c] = rep_disp[c].astype(float).round(4)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8.5, 3.8), dpi=dpi)\n",
    "    ax.set_title(f\"{title}\\nClassification Report (Test Set)\", fontsize=13, fontweight=\"bold\", pad=12)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    table = ax.table(\n",
    "        cellText=rep_disp.values,\n",
    "        rowLabels=[str(i).upper() for i in rep_disp.index],\n",
    "        colLabels=[c.upper() for c in rep_disp.columns],\n",
    "        cellLoc=\"center\",\n",
    "        rowLoc=\"center\",\n",
    "        loc=\"center\"\n",
    "    )\n",
    "\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 1.4)\n",
    "\n",
    "    # Light emphasis for header row/col\n",
    "    for (row, col), cell in table.get_celld().items():\n",
    "        if row == 0:\n",
    "            cell.set_text_props(fontweight=\"bold\")\n",
    "        if col == -1:\n",
    "            cell.set_text_props(fontweight=\"bold\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save:\n",
    "        out = exp_dir / \"plot_classification_report.png\"\n",
    "        fig.savefig(out, bbox_inches=\"tight\")\n",
    "    return fig\n",
    "\n",
    "def plot_all_for_experiment(exp_dir, labels=(\"normal\", \"crackle\", \"wheeze\"), dpi=300):\n",
    "    exp_dir = Path(exp_dir)\n",
    "    figs = []\n",
    "    figs.append(plot_train_curves(exp_dir, save=True, dpi=dpi))\n",
    "    figs.append(plot_confusion_matrix(exp_dir, labels=labels, normalize=None, save=True, dpi=dpi))\n",
    "    figs.append(plot_confusion_matrix(exp_dir, labels=labels, normalize=\"true\", save=True, dpi=dpi))\n",
    "    figs.append(plot_classification_report(exp_dir, save=True, dpi=dpi))\n",
    "    return figs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d732d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# of alles in een loop\n",
    "for d in runs:\n",
    "    plot_all_for_experiment(d, labels=[\"normal\", \"crackle\", \"wheeze\"], dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc1b515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
